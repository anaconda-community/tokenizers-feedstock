{% set name = "tokenizers" %}
{% set version = "0.13.1" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz
  sha256: 3333d1cee5c8f47c96362ea0abc1f81c77c9b92c6c3d11cbf1d01985f0d5cf1d

build:
  number: 0
  missing_dso_whitelist:
    - /usr/lib/libresolv.9.dylib  # [osx]
    - /usr/lib64/libgcc_s.so.1  # [linux]
  script:
    - export PYO3_CROSS_LIB_DIR={{ PREFIX }}/lib  # [arm64]
    - find {{ PREFIX }}/lib/python3.* -name "_sysconfigdata_*.py" -type f -not -name "_sysconfigdata_arm64_*.py" -delete  # [arm64]
    - {{ PYTHON }} -m pip install . -vv

requirements:
  build:
    - python                                 # [build_platform != target_platform]
    - cross-python_{{ target_platform }}     # [build_platform != target_platform]
    - {{ compiler('cxx') }}
    - {{ compiler('rust') }}
  host:
    - python
    - pip
    - setuptools-rust >=0.11.5
    - setuptools
    - openssl    # [linux]
  run:
    - python

test:
  imports:
    - tokenizers
    - tokenizers.models
    - tokenizers.decoders
    - tokenizers.normalizers
    - tokenizers.pre_tokenizers
    - tokenizers.processors
    - tokenizers.trainers
    - tokenizers.implementations
  commands:
    - pip check
  requires:
    - pip

about:
  home: https://pypi.org/project/tokenizers/
  license: Apache-2.0
  license_family: APACHE
  license_file: LICENSE
  summary: Fast State-of-the-Art Tokenizers optimized for Research and Production
  dev_url: https://github.com/huggingface/tokenizers

extra:
  recipe-maintainers:
    - anthchirp
    - ndmaxar
    - oblute
    - setu4993
